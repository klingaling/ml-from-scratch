{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node class to store all nodes and leafs in the tree\n",
    "class Node:\n",
    "    def __init__(self, feature = None, categories = [], threshold = None, value= None, children = [], label_mode = None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.categories = categories\n",
    "        self.value = value\n",
    "        self.children = children\n",
    "        self.label_mode = label_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split criterium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computes the squared error of each split, for identifying the MSE\n",
    "def squared_error (y):\n",
    "    estimate = np.mean(y)\n",
    "    return [(actual-estimate)**2 for actual in y]\n",
    "\n",
    "#calculates and returns the entropy of a certain split of a feature given the class frequencies\n",
    "def entropy (y):\n",
    "    class_counts = np.bincount(y)\n",
    "    class_frequency = class_counts/len(y)\n",
    "    class_frequency = [freq for freq in class_frequency if freq>0]\n",
    "    return -np.sum((class_frequency * np.log2(class_frequency)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stop criterium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop criteria for regression \n",
    "def stop_regression (current_depth, max_depth, early_stop, mse):\n",
    "    if (current_depth==max_depth or mse <= early_stop):\n",
    "        return True\n",
    "    else:\n",
    "        return False   \n",
    "#stop criteria for classification    \n",
    "def stop (y, current_depth, max_depth, min_split, labels):\n",
    "    if ( current_depth==max_depth or len(labels)==1):#len(y) < min_split or\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split by data type and model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_splitting_classification (y, feature_vals):\n",
    "    #calculates the entropy at the root of this subtree\n",
    "    root_entropy = entropy(y)\n",
    "    #get the list of the unique categories for the feature and append them\n",
    "    unique_feature_vals = np.unique(feature_vals)\n",
    "    \n",
    "    weighted_gain = 0\n",
    "    y_args = []\n",
    "    entropies = []\n",
    "    #iterate through each categorical feature value\n",
    "    for feature_val in unique_feature_vals:\n",
    "        #get the indices for all the samples with this value\n",
    "        feature_args = np.argwhere(feature_vals == feature_val).flatten()\n",
    "        y_args.append(feature_args)\n",
    "        #calculate the frequency of this feature value\n",
    "        feature_frequency = len(feature_args)/len(feature_vals)\n",
    "        #get the y values for each index of this feature value\n",
    "        feature_val_y_vals = y[feature_args]\n",
    "        #calculates the entropy for this feature value and multiplies it by its frequency to weight it\n",
    "        entropies.append(feature_frequency*entropy(feature_val_y_vals))\n",
    "        #add to weighted gain list for gain ratio\n",
    "        weighted_gain += (feature_frequency*np.log2(feature_frequency))\n",
    "    #append the list of the indices for each feature value for splitting in the future\n",
    "    #feature_val_y_args_list.append(y_args)\n",
    "    #calculate the feature's gain and add it to the list\n",
    "    feature_gain = (root_entropy-np.sum(entropies))/(-weighted_gain)\n",
    "    #feature_gains.append(feature_gain)\n",
    "    #categories.append(unique_feature_vals)\n",
    "    #threshold.append(None)\n",
    "    metric_dict = {\n",
    "        'y_args': y_args,\n",
    "        'gain': feature_gain,\n",
    "        'categories': unique_feature_vals,\n",
    "        'threshold': None\n",
    "    }\n",
    "    return metric_dict\n",
    "def numerical_splitting_classification (y, feature_vals):\n",
    "    #calculates the entropy at the root of this subtree\n",
    "    root_entropy = entropy(y)\n",
    "    #print(list(feature_vals))\n",
    "    #get the list of the unique categories for the feature and append them\n",
    "    unique_feature_vals = np.unique(feature_vals)\n",
    "    #we only need the values for the feature with the optimal threshold, so we use placeholders\n",
    "    best_gain = -1\n",
    "    best_y_args = None\n",
    "    best_threshold = None\n",
    "    best_category = None\n",
    "    #iterate through each unique feature value and use it as the threshold\n",
    "    for feature_val in unique_feature_vals:\n",
    "        weighted_gain = 0\n",
    "        category = None\n",
    "        y_args = [np.argwhere(feature_vals < feature_val).flatten(), np.argwhere(feature_vals >= feature_val).flatten()]\n",
    "        threshold = feature_val\n",
    "        #split the data by the threshold\n",
    "        left = y[y_args[0]]\n",
    "        right = y[y_args[1]]\n",
    "        #temp list for entropies of each side\n",
    "        entropies = []\n",
    "        #iterate through both sides and calculate the entropies for each\n",
    "        for side in [left, right]:\n",
    "            feature_frequency = len(side)/len(feature_vals)\n",
    "            entropies.append(feature_frequency*entropy(side))\n",
    "            weighted_gain+= (feature_frequency*np.log2(feature_frequency))\n",
    "        #calculate the gain for this threshold's split\n",
    "        gain = root_entropy-np.sum(entropies)/(-weighted_gain)\n",
    "        #if this gain is higher than our current best, replace it with this one\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_y_args = y_args\n",
    "            best_threshold = threshold\n",
    "            best_category = category\n",
    "    metric_dict = {\n",
    "        'y_args': best_y_args,\n",
    "        'gain': best_gain,\n",
    "        'categories': best_category,\n",
    "        'threshold': best_threshold\n",
    "    }\n",
    "    return metric_dict \n",
    "\n",
    "def splitting_regression(y, feature_vals):\n",
    "    unique_feature_vals = np.unique(feature_vals)\n",
    "    best_mse = 99999999999\n",
    "    best_y_args = None\n",
    "    best_threshold = None\n",
    "    for feature_val in unique_feature_vals:\n",
    "        y_args = [np.argwhere(feature_vals < feature_val).flatten(), np.argwhere(feature_vals >= feature_val).flatten()]\n",
    "        #split the data by the threshold\n",
    "        left = y[y_args[0]]\n",
    "        right = y[y_args[1]]\n",
    "        if((len(left)!=0 and len(right)!=0)):\n",
    "            split_mse = np.mean(squared_error(left) + squared_error(right))\n",
    "            if best_mse == 99999999999 or split_mse<best_mse:\n",
    "                best_mse = split_mse\n",
    "                best_y_args = y_args\n",
    "                best_threshold = feature_val\n",
    "    metric_dict = {\n",
    "        'y_args': best_y_args,\n",
    "        'mse': best_mse,\n",
    "        'threshold': best_threshold\n",
    "    }\n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# execute splits for fitting tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to grow the regression tree. it runs recursively on the samples and as we split, the samples split with it\n",
    "#it returns the root of the tree\n",
    "def execute_regression_split (x, y, max_depth, depth, early_stop, mse):\n",
    "    #return a leaf if we should stop \n",
    "    if (stop_regression(depth, max_depth, early_stop, mse)):\n",
    "        leaf_val = np.median(y)\n",
    "        return Node(value = leaf_val)\n",
    "    n_samples, feature_count = x.shape\n",
    "    feature_mses = []\n",
    "    feature_val_y_args_list = []\n",
    "    threshold = []\n",
    "    #iterate through each feature in the samples\n",
    "    for feature_index in range(0, feature_count):\n",
    "        feature_vals = x[:, feature_index]\n",
    "        if (len(np.unique(feature_vals))!=1):\n",
    "            feature_metrics = splitting_regression(y, feature_vals)\n",
    "            #append our optimal threshold split to the main list \n",
    "            feature_val_y_args_list.append(feature_metrics['y_args'])\n",
    "            feature_mses.append(feature_metrics['mse'])\n",
    "            threshold.append(feature_metrics['threshold'])\n",
    "        else:\n",
    "            feature_val_y_args_list.append([])\n",
    "            feature_mses.append(999999999)\n",
    "            threshold.append(0)\n",
    "    \n",
    "    best_feature_split = np.argmin(feature_mses)\n",
    "    best_feature_val_y_args = feature_val_y_args_list[best_feature_split]\n",
    "    best_threshold = threshold[best_feature_split]\n",
    "    if (len(best_feature_val_y_args)==0):\n",
    "        leaf_val = np.median(y)\n",
    "        return Node(value = leaf_val)\n",
    "    depth+=1\n",
    "    #recursively call this function on the two sides of the optimal split\n",
    "    children = [execute_regression_split(x[feature_args, :], y[feature_args], max_depth, depth, early_stop, feature_mses[best_feature_split]) for feature_args in best_feature_val_y_args]\n",
    "    return Node(feature = best_feature_split, children = children, threshold = best_threshold)\n",
    "\n",
    "#this function is used to grow the classification tree. it runs recursively on the samples and as we split, the samples split with it\n",
    "#it returns the root of the tree\n",
    "def execute_split (x, y, feature_count, depth):\n",
    "    labels,counts = np.unique(y,return_counts = True)\n",
    "    label_mode = labels[np.argmax(counts)]\n",
    "    #returns a leaf node if we have reached a stopping point\n",
    "    if (stop(y, depth, max_depth = 10, min_split = 2, labels = labels)):\n",
    "        leaf_val = labels[np.argmax(counts)]\n",
    "        return Node(value = leaf_val)\n",
    "    \n",
    "    #place holders for the gains of each feature, the list of each value for the feature, and the indices of the y vals for each feature\n",
    "    feature_gains = []\n",
    "    feature_val_y_args_list = []\n",
    "    categories = []\n",
    "    threshold = []\n",
    "    #iterate through each feature in the samples\n",
    "    for feature_index in range(0, feature_count):\n",
    "        feature_vals = x[:, feature_index]\n",
    "        feature_val_entropies = []\n",
    "        #if else to treat categorical and numerical data differently\n",
    "        if all(isinstance(val, str) for val in feature_vals):\n",
    "            feature_metrics = categorical_splitting_classification(y, feature_vals)\n",
    "        #numeric\n",
    "        else:\n",
    "            feature_metrics = numerical_splitting_classification(y, feature_vals)\n",
    "        #append our optimal threshold split to the main list \n",
    "        feature_val_y_args_list.append(feature_metrics['y_args'])\n",
    "        feature_gains.append(feature_metrics['gain'])\n",
    "        categories.append(feature_metrics['categories'])\n",
    "        threshold.append(feature_metrics['threshold'])\n",
    "    #identify best split\n",
    "    best_feature_split = np.argmax(feature_gains)\n",
    "    best_feature_val_y_args = feature_val_y_args_list[best_feature_split]\n",
    "    best_categories = categories[best_feature_split]\n",
    "    best_threshold = threshold[best_feature_split]\n",
    "    depth+=1\n",
    "    #recursively call this function on the x children of the optimal split\n",
    "    children = [execute_split(x[feature_args, :], y[feature_args], feature_count, depth) for feature_args in best_feature_val_y_args]\n",
    "\n",
    "    return Node(feature = best_feature_split, children = children, threshold = best_threshold, categories = best_categories, label_mode = label_mode)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fits the decision tree regressor, just runs the above function and returns the root\n",
    "def fit_decision_tree_regressor (x, y, max_depth = None, p_features = None, early_stop = None):\n",
    "    root = execute_regression_split(x, y, max_depth = 10, depth=0, early_stop=early_stop, mse=999999999)\n",
    "    return root\n",
    "#fits the decision tree classifier, just runs the above function and returns the root\n",
    "def fit_decision_tree (x, y, max_depth = None, p_features = None, min_split = None):\n",
    "    n_samples, n_features = x.shape\n",
    "    root = execute_split(x, y, n_features, depth=0)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterates through each sample and runs it through the predict split method below\n",
    "def predict_regression (x, root):\n",
    "    predictions = []\n",
    "    for sample in x:\n",
    "        predictions.append(predict_split_regression(sample, root))\n",
    "    return predictions\n",
    "\n",
    "#recursive method that traverses through the fitted regression tree \n",
    "def predict_split_regression (sample, root):\n",
    "    if str(root.value).replace('.','').isnumeric():\n",
    "        return root.value\n",
    "    else:\n",
    "        current_feature = sample[root.feature]\n",
    "        children = root.children\n",
    "        if current_feature< root.threshold:\n",
    "            return predict_split_regression(sample, children[0])\n",
    "        else:\n",
    "            return predict_split_regression(sample, children[1])\n",
    "\n",
    "#iterates through each sample and runs it through the predict split method below\n",
    "def predict_clf (x, root):\n",
    "    predictions = []\n",
    "    for sample in x:\n",
    "        predictions.append(predict_split_clf(sample, root))\n",
    "    return predictions\n",
    "\n",
    "#recursive method that traverses through the fitted regression tree \n",
    "def predict_split_clf (sample, root):\n",
    "    if str(root.value).isnumeric():\n",
    "        return root.value\n",
    "    else:\n",
    "        current_feature = sample[root.feature]\n",
    "        children = root.children\n",
    "        if isinstance(current_feature, str):\n",
    "            for index, category in enumerate(root.categories):\n",
    "                if current_feature == category:\n",
    "                    return predict_split_clf(sample, children[index])\n",
    "            #categorical labels can potentially be dropped when fitting the tree as we filter the subsets\n",
    "            #if this happens, we should return the mode label at that subroot\n",
    "            return root.label_mode\n",
    "        else:\n",
    "            if current_feature< root.threshold:\n",
    "                return predict_split_clf(sample, children[0])\n",
    "            else:\n",
    "                return predict_split_clf(sample, children[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduced error pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this takes in the fitted tree and validation set and prunes the classification tree\n",
    "#it does this by testing the original accuracy vs the pruined accuracy, if the pruned accuracy is higher, keep the prune\n",
    "def prune_tree(root, current_node, x, y, x_validation, y_validation):\n",
    "    if current_node.value == None:\n",
    "        current_feature = current_node.feature\n",
    "        feature_vals = x[:, current_feature]\n",
    "        unique_feature_vals = np.unique(feature_vals)\n",
    "        #current accuracy\n",
    "        y_predict = predict_clf (x_validation, root)\n",
    "        results = [y_predict[result_index]==actual for result_index, actual in enumerate(y_validation)]\n",
    "        current_accuracy = results.count(True)/len(results)\n",
    "        #compute pruined accuracy\n",
    "        node_threshold = current_node.threshold\n",
    "        node_categories = current_node.categories\n",
    "        labels,counts = np.unique(y,return_counts = True)\n",
    "        leaf_val = labels[np.argmax(counts)]\n",
    "        current_node.value = leaf_val\n",
    "        pruned_precict = predict_clf(x_validation, root)\n",
    "        pruned_results = [pruned_precict[result_index]==actual for result_index, actual in enumerate(y_validation)]\n",
    "        pruned_accuracy = pruned_results.count(True)/len(pruned_results)\n",
    "        \n",
    "        if pruned_accuracy >= current_accuracy:\n",
    "            return root\n",
    "        else:\n",
    "            #set the value of the node back to None to make it a node again\n",
    "            current_node.value = None\n",
    "            if isinstance(feature_vals[0], str):\n",
    "                feature_args = [np.argwhere(feature_vals == node_category).flatten() for node_category in node_categories ]\n",
    "            else:\n",
    "                feature_args = [np.argwhere(feature_vals < node_threshold).flatten(), np.argwhere(feature_vals >= node_threshold).flatten()]\n",
    "            #recursively call the method on each child\n",
    "            for index, child in enumerate(current_node.children):\n",
    "                split_arg = feature_args[index]\n",
    "                if len(feature_args[index])>0:\n",
    "                    root = prune_tree(root, child, x[split_arg, :], y[split_arg], x_validation, y_validation)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into 5 different sets and return each set, also allows for the creation of a validation set\n",
    "def validation_sets(data, classification, optimize = False):\n",
    "    sets = []\n",
    "    sample_size = len(data)\n",
    "    if optimize:\n",
    "        if classification:\n",
    "            validation_set = data.groupby('target').apply(lambda x: x.sample(frac = 0.1)).reset_index(level=0, drop=True)\n",
    "        else:\n",
    "            validation_set = data.sample(n=int(sample_size*0.1))\n",
    "        sample_size = len(data) - len(validation_set)\n",
    "        data = data.drop(validation_set.index)\n",
    "    else:\n",
    "        validation_set = None\n",
    "    if classification:\n",
    "        class_splits = []\n",
    "        for class_val in np.unique(data['target']):\n",
    "            df_class = data[data['target'] == class_val]\n",
    "            dfs_class = np.array_split(df_class, 5)\n",
    "            class_splits.append(dfs_class)\n",
    "    for index in range(0, 5):\n",
    "        #for classification, make sure each class has equal representation for each set as they do in the full dataset\n",
    "        if classification:\n",
    "            sample_set_list = []\n",
    "            for class_index in range(len(class_splits)):\n",
    "                sample_set_list.append(class_splits[class_index][index])\n",
    "            sample_set = pd.concat(sample_set_list)\n",
    "        #we cant do the above for regression, so just split the data normally \n",
    "        else:\n",
    "            sample_set = data.sample(n=int(sample_size*0.2))\n",
    "            data = data.drop(sample_set.index)\n",
    "        sets.append(sample_set.reset_index().drop(columns= 'index'))\n",
    "    return sets, validation_set\n",
    "\n",
    "#this is the shell for all our model. it takes each set and tests it against the rest of the sets and returns some metrics we need \n",
    "#to evaluate our model. it also allows for pruning for classification, and optimizing an MSE value for early stopping for regression\n",
    "#these require validation sets.\n",
    "def k_fold_cross_validation(sets, classification = True, validation_set = None, pruning = False,  mse_list = [0.2]):\n",
    "    scores = []\n",
    "    #iterate through each set as the test set and calculate metrics\n",
    "    if pruning or len(mse_list) > 1:\n",
    "        y_validation = np.array(validation_set['target'])\n",
    "        x_validation = validation_set.reset_index().drop(columns= ['index','target']).values\n",
    "    if len(mse_list)>1:\n",
    "        training_set = pd.concat(sets)\n",
    "        y_train = np.array(training_set['target'])\n",
    "        #drop target so its not considered a feature\n",
    "        x_train = training_set.reset_index().drop(columns= ['index','target']).values\n",
    "        best_result = None\n",
    "        best_mse = None\n",
    "        for mse in mse_list:\n",
    "            root = fit_decision_tree_regressor(x_train, y_train, max_depth = None, p_features = None, early_stop = mse)\n",
    "            #runs the data through our regression model\n",
    "            y_predict_validation = predict_regression(x_validation, root)\n",
    "            result_rsquared = 1- (((y_validation-y_predict_validation)**2).sum(axis=0)/((y_validation-np.average(y_validation, axis=0,))**2).sum(axis=0))\n",
    "            if best_result == None or result_rsquared >= best_result:\n",
    "                best_mse= mse\n",
    "                best_result = result_rsquared\n",
    "    else:\n",
    "        best_mse = mse_list[0]\n",
    "    \n",
    "    for index in range(0, len(sets)):\n",
    "        test_set = sets[index]\n",
    "        #concats the rest of the sets for training\n",
    "        training_set = pd.concat([t_set for (set_index, t_set) in enumerate(sets) if set_index!=index])\n",
    "        y_train = np.array(training_set['target'])\n",
    "        #drop target so its not considered a feature\n",
    "        x_train = training_set.reset_index().drop(columns= ['index','target']).values\n",
    "\n",
    "        y_test = np.array(test_set['target'])\n",
    "        x_test = test_set.reset_index().drop(columns= ['index','target']).values\n",
    "        \n",
    "        #here we either choose to treat the data as classification or regression\n",
    "        if classification:\n",
    "            root = fit_decision_tree (x_train, y_train, max_depth = None, p_features = None, min_split = None)\n",
    "            if pruning == True:\n",
    "                root = prune_tree(root, root, x_validation, y_validation, x_validation, y_validation,)\n",
    "            y_predict = predict_clf (x_test, root)\n",
    "            results = [y_predict[result_index]==actual for result_index, actual in enumerate(y_test)]\n",
    "            scores.append(results.count(True)/len(results))\n",
    "        else:\n",
    "            \n",
    "            root = fit_decision_tree_regressor(x_train, y_train, max_depth = None, p_features = None, early_stop = best_mse)\n",
    "            y_predict = predict_regression(x_test, root)\n",
    "            avg_deviation = np.mean([(np.abs(y_predict[i]-y_test[i]))/(y_test[i]) for i in range(0, len(y_test))])\n",
    "            result_mse = np.square(y_test - y_predict).mean()\n",
    "            rsquared = 1- (((y_test-y_predict)**2).sum(axis=0)/((y_test-np.average(y_test, axis=0,))**2).sum(axis=0))\n",
    "            scores.append([best_mse, result_mse, rsquared, avg_deviation])\n",
    "    if classification:\n",
    "        average_results = np.mean(scores)\n",
    "    else:\n",
    "        average_results = []\n",
    "        for index in range(len(scores)-1):\n",
    "            average_results.append(np.mean([scores[0][index], scores[1][index],scores[2][index],scores[3][index]]))\n",
    "    return average_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation = pd.read_csv('segmentation.data')\n",
    "segmentation = segmentation.reset_index().rename(columns = {'index': 'target'})\n",
    "segmentation = segmentation.drop(columns = 'REGION-PIXEL-COUNT')\n",
    "labels = np.unique(segmentation['target'])\n",
    "segmentation['target'] = [np.where(labels == label)[0][0] for label in segmentation['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:63: RuntimeWarning: divide by zero encountered in log2\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:63: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8714285714285713"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sets, validation_set = validation_sets(segmentation, classification = True, optimize = False)\n",
    "k_fold_cross_validation(sets, classification= True, validation_set = None, pruning = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:63: RuntimeWarning: divide by zero encountered in log2\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:63: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.781904761904762"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sets, validation_set = validation_sets(segmentation, classification = True, optimize = True)\n",
    "k_fold_cross_validation(sets, classification= True, validation_set = validation_set, pruning = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone = pd.read_csv('abalone.data', header = None)\n",
    "abalone.columns = ['sex', 'length', 'diameter', 'height', 'whole_weight', 'shucked_weight', 'viscera_weight', 'shell_weight', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:63: RuntimeWarning: divide by zero encountered in log2\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:63: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24204419336942898"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sets, validation_set = validation_sets(abalone, classification = True, optimize = False)\n",
    "k_fold_cross_validation(sets, classification= True, validation_set = None, pruning = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:63: RuntimeWarning: divide by zero encountered in log2\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:63: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24259453644033124"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sets, validation_set = validation_sets(abalone, classification = True, optimize = True)\n",
    "k_fold_cross_validation(sets, classification= True, validation_set = validation_set, pruning = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('car.data', header = None)\n",
    "cars.columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars['doors'] = pd.to_numeric(cars['doors'].replace('5more', 5))\n",
    "cars['persons'] = pd.to_numeric(cars['persons'].replace('more', 6))\n",
    "cars['target'] = pd.to_numeric(cars['target'].replace('unacc', 0).replace('acc', 1).replace('good', 2).replace('vgood', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:63: RuntimeWarning: divide by zero encountered in log2\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:63: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7002352466729399"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sets, validation_set = validation_sets(cars, classification = True, optimize = False)\n",
    "k_fold_cross_validation(sets, classification= True, validation_set = None, pruning = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:63: RuntimeWarning: divide by zero encountered in log2\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:63: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7005186185037007"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sets, validation_set = validation_sets(cars, classification = True, optimize = True)\n",
    "k_fold_cross_validation(sets, classification= True, validation_set = validation_set, pruning = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "machines = pd.read_csv('machine.data', header=None)\n",
    "machines.columns = ['vendor', 'model', 'myct', 'mmin', 'mmax', 'cach', 'chmin', 'chmax', 'target', 'erp']\n",
    "machines = machines.drop(columns = ['model', 'erp'])\n",
    "machines = machines.drop(columns = 'vendor') #.join(pd.get_dummies(machines['vendor']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1000.0, 4704.387195121952, 0.8138258215609964, 0.5368478179743618]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sets, validation_set = validation_sets(machines, classification = False)\n",
    "k_fold_cross_validation(sets, classification = False, mse_list = [1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest fires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_fires = pd.read_csv('forestfires.data')\n",
    "forest_fires = forest_fires.rename(columns = {'area': 'target'})\n",
    "forest_fires['target'] = forest_fires['target']#.apply(lambda x: np.log(x+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change month and days to #s\n",
    "months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "days = ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']\n",
    "forest_fires['day'] = forest_fires['day'].apply(lambda x: days.index(x) if str(x).isalpha() else x)\n",
    "forest_fires['month'] = forest_fires['month'].apply(lambda x: months.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:84: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4000.0, 5049.758879733008, -0.17698036753428564, nan]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sets, validation_set = validation_sets(forest_fires, classification = False)\n",
    "k_fold_cross_validation(sets, classification = False, mse_list = [4000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine quality - red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_quality_red = pd.read_csv('winequality-red.csv', delimiter = ';').rename(columns = {'quality': 'target'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4, 0.6130485893416928, 0.07839679272834488, 0.08867181668905808]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sets, validation_set = validation_sets(wine_quality_red, classification = False)\n",
    "k_fold_cross_validation(sets, classification = False, mse_list = [0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine quality - white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_quality_white = pd.read_csv('winequality-white.csv', delimiter = ';').rename(columns = {'quality': 'target'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets, validation_set = validation_sets(wine_quality_white, classification = False)\n",
    "k_fold_cross_validation(sets, classification = False, mse_list = [0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
