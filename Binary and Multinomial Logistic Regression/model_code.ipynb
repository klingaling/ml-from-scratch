{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class that contains all the tools to run both binary and multinomial logistic regressions\n",
    "class LogisticRegression():\n",
    "    \n",
    "    #initializes whether classes are binary or not, which preprocessing we would like to do\n",
    "    #what our learning rate will be, and how many iterations\n",
    "    def __init__ (self, binary = True, lr_preprocess=None, learning_rate = 0.01, iterations = 100):\n",
    "        self.learning_rate = learning_rate    \n",
    "        self.iterations = iterations\n",
    "        self.binary = binary\n",
    "        self.lr_preprocess = lr_preprocess\n",
    "    \n",
    "    #function that preprocesses features by standardizing or normalizing them and returns the preprocessed features\n",
    "    #if is None, just return the raw features\n",
    "    def preprocess_features(self, x):\n",
    "        if self.lr_preprocess == 'standardize':\n",
    "            return (x-np.mean(x,axis=0))/np.std(x,axis= 0)\n",
    "        elif self.lr_preprocess == 'normalize':\n",
    "            return (((x-np.min(x, axis=0))/(np.max(x, axis=0)-np.min(x, axis=0)))*-1)+1\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    #this calculates and returns the current outputs of our linear function given the current weight and bias\n",
    "    #used for binary classification\n",
    "    def calc_linear_function(self, x):\n",
    "        return np.dot(x, self.weight) + self.bias\n",
    "    \n",
    "    #this calculates and returns the current outputs of our linear functions for each class given the current weight and bias\n",
    "    #used for multinomial/multiclass\n",
    "    def calc_multiclass_linear_functions(self, x):\n",
    "        return np.dot(x, self.weights.T) + self.biases.T\n",
    "    \n",
    "    #this function transforms our labels into a one hot encoded version of the labels, it returns this as the new y labels\n",
    "    #this is used for multinomial/multiclass\n",
    "    def one_hot(self, y, classes):\n",
    "        one_hot_array = np.zeros(shape= (len(y), len(classes)))\n",
    "        for index, y_val in enumerate(y):\n",
    "            one_hot_array[index][np.argwhere(classes==y_val)]=1\n",
    "        return one_hot_array\n",
    "    \n",
    "    #we use the the softmax function to estimate the probabilities of a sample being part of a certain class for multinomial/multiclass problems \n",
    "    def softmax(self, linear_function):\n",
    "        return np.exp(linear_function)/np.sum(np.exp(linear_function),axis=1).reshape(-1,1)\n",
    "    \n",
    "    #we use the sigmoid function to estimate the probabilities of a sample being part of a certain class for binary classification\n",
    "    def sigmoid(self, linear_function):\n",
    "        return 1/(1+ np.exp(-linear_function))\n",
    "    \n",
    "    #this is the fit function for logistic regression. it accepts the training data and calculates the optimal weights and biases given the labels\n",
    "    #first the data is preprocessed when set to do so. then we initialize the weights... for multiclass we one hot encode the labels and initialize weights\n",
    "    #for each of the classes. next we iterate the set number of times, calculate the linear function(s), the predicted y values, the weight and bias\n",
    "    #gradients, and update the weights by subtracting the gradient*learning rate.\n",
    "    #we find the optimal weight using gradient descent on the sigmoid function for binary and softmax function for multiclass\n",
    "    def fit(self, x, y):\n",
    "        sample_count, feature_count = x.shape\n",
    "        x = self.preprocess_features(x)\n",
    "        if self.binary:\n",
    "            self.weight = np.random.uniform(low=-0.01, high=0.01, size=feature_count)#np.zeros(feature_count)\n",
    "            self.bias = np.random.uniform (low=-0.01, high=0.01)#0\n",
    "        else:\n",
    "            classes = np.unique(y)\n",
    "            y = self.one_hot(y, classes)\n",
    "            self.weights = np.zeros(shape = (len(classes), feature_count))\n",
    "            self.biases = np.zeros(len(classes))\n",
    "        for run in range(self.iterations):\n",
    "            if self.binary:\n",
    "                linear_function = self.calc_linear_function(x)\n",
    "                y_predict = self.sigmoid(linear_function)\n",
    "                weight_gradient = np.dot(x.T, y_predict - y)#*(1/sample_count)\n",
    "                bias_gradient = np.sum(y_predict - y)#/sample_count\n",
    "                self.weight -= weight_gradient*self.learning_rate\n",
    "                self.bias -= bias_gradient*self.learning_rate\n",
    "            else:\n",
    "                linear_functions = self.calc_multiclass_linear_functions(x)\n",
    "                y_predict = self.softmax(linear_functions)\n",
    "                weight_gradient = np.dot((y_predict - y).T, x)#*(1/sample_count)\n",
    "                bias_gradient = np.sum((y_predict - y).T)#/sample_count\n",
    "                self.weights -= weight_gradient*self.learning_rate\n",
    "                self.biases -= bias_gradient*self.learning_rate\n",
    "\n",
    "    #this is the prediction method for logistic regression. we follow the same guidelines for calculating the predicted y value as the fit method\n",
    "    #using sigmoid for binary and softmax for multiclass. for binary, we predict 1 if the predicted value is greater than 0.5, 0 else.\n",
    "    #for multiclass we take the argmax of the predicted vals and set that as our predicted class\n",
    "    def predict(self, x):\n",
    "        x = self.preprocess_features(x)\n",
    "        if self.binary:\n",
    "            linear_function = self.calc_linear_function(x)\n",
    "            y_predict = self.sigmoid(linear_function)\n",
    "            return [1 if y > 0.5 else 0 for y in y_predict]\n",
    "        else:\n",
    "            linear_functions = self.calc_multiclass_linear_functions(x)\n",
    "            y_predict = self.softmax(linear_functions)\n",
    "            return np.argmax(y_predict, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class that contains all the tools to run both binary and multinomial adalines\n",
    "class Adaline():\n",
    "    \n",
    "    #initializes whether classes are binary or not, which preprocessing we would like to do\n",
    "    #what our learning rate will be, and how many iterations    \n",
    "    def __init__(self, binary=True, adaline_preprocess=None, learning_rate = 0.01, iterations = 100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "        self.weight_temp = None\n",
    "        self.bias_temp = None\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.classes = None\n",
    "        self.binary = binary\n",
    "        self.adaline_preprocess = adaline_preprocess\n",
    "        \n",
    "    #function that preprocesses features by standardizing or normalizing them and returns the preprocessed features\n",
    "    #if adaline_preprocess is None, just return the raw features\n",
    "    def preprocess_features(self, x):\n",
    "        if self.adaline_preprocess == 'standardize':\n",
    "            return (x-np.mean(x,axis=0))/np.std(x,axis= 0)\n",
    "        elif self.adaline_preprocess == 'normalize':\n",
    "            return (((x-np.min(x, axis=0))/(np.max(x, axis=0)-np.min(x, axis=0)))*-1)+1\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    #this calculates and returns the current outputs of our linear function given the current weight and bias\n",
    "    #used for binary classification\n",
    "    def calc_linear_function(self, x):\n",
    "        return np.dot(x, self.weight) + self.bias\n",
    "\n",
    "    #the activation function of adaline is linear so we don't alter it in any way\n",
    "    def activation_function(self, z):\n",
    "        return z\n",
    "    \n",
    "    #this function is used to calculate the cost of each prediction, we want this to decrease over iterations\n",
    "    def cost_function(self, y, y_predict):\n",
    "        cost = np.sum((y_predict-y)**2)/2.0\n",
    "        return cost\n",
    "    \n",
    "    #this is the fit function for adaline. it accepts the training data and calculates the optimal weights and biases given the labels\n",
    "    #first the data is preprocessed when set to do so. then we initialize the weights to some random number close to 0. next we iterate \n",
    "    #the set number of times, for binary, we calculate the linear function(s), put the linear function through an activation function \n",
    "    #which just returns the same thing for adaline, calculate the predicted y values, calculate the weight and bias\n",
    "    #gradients, and update the weights and bias by subtracting the gradient*learning rate. for multiclass we use a one-v-all\n",
    "    #approach, relabeling the data so one class is 1 and the rest are 0, and updating the weights and biases for each of the classes for each iteration\n",
    "    #to find the optimal weights and bias we use gradient descent to minimize the sum of squared errors cost function\n",
    "    def fit(self, x, y):\n",
    "        sample_count, feature_count = x.shape\n",
    "        weight_master = list(np.random.uniform(low=-0.01, high=0.01, size=feature_count)).copy()\n",
    "        bias_master =np.random.uniform (low=-0.01, high=0.01)\n",
    "        self.weight = weight_master \n",
    "        self.bias = bias_master\n",
    "        x = self.preprocess_features(x)\n",
    "        \n",
    "        #print(x)\n",
    "        if self.binary:\n",
    "            for run in range(self.iterations):\n",
    "                linear_function = self.calc_linear_function(x)\n",
    "                y_predict = self.activation_function(linear_function)\n",
    "                weight_gradient = np.dot(x.T, y_predict-y)\n",
    "                bias_gradient = (y_predict-y).sum()\n",
    "                self.weight -= weight_gradient*self.learning_rate\n",
    "                self.bias -= bias_gradient*self.learning_rate\n",
    "        else:\n",
    "            self.classes = np.unique(y)\n",
    "            for class_val in self.classes:\n",
    "                self.weight = weight_master\n",
    "                self.bias = bias_master\n",
    "                y_temp = [1 if y_val==class_val else 0 for y_val in y]\n",
    "                for run in range(self.iterations):\n",
    "                    linear_function = self.calc_linear_function(x)\n",
    "                    y_predict = self.activation_function(linear_function)\n",
    "                    sampl = np.random.randint(0, len(y))\n",
    "                    weight_gradient = x.T.dot(y_predict-y_temp)\n",
    "                    bias_gradient = np.sum(y_predict-y_temp)\n",
    "                    self.weight -= weight_gradient*self.learning_rate\n",
    "                    self.bias -= bias_gradient*self.learning_rate\n",
    "                self.weights.append(self.weight)\n",
    "                self.biases.append(self.bias)\n",
    "                \n",
    "    #this is the prediction method for adaline. we follow the same guidelines for calculating the predicted y value as the fit method\n",
    "    #for binary, we predict 1 if the predicted value is greater than 0, 0 else.\n",
    "    #for multiclass we take the argmax of the y_predicts which act as conand set that as our predicted class.\n",
    "    def predict(self, x):\n",
    "        onevall_predictions = []\n",
    "        predictions = []\n",
    "        x = self.preprocess_features(x)\n",
    "        if self.binary:\n",
    "            linear_function = self.calc_linear_function(x)\n",
    "            y_predict = self.activation_function(linear_function)\n",
    "            return [1 if y_val >=0 else 0 for y_val in y_predict]\n",
    "        else:\n",
    "            for index, class_val in enumerate(self.classes):\n",
    "                self.weight = self.weights[index]\n",
    "                self.bias = self.biases[index]\n",
    "                linear_function = self.calc_linear_function(x)\n",
    "                y_predict = self.activation_function(linear_function)\n",
    "                onevall_predictions.append(y_predict)\n",
    "            for sample in range(len(onevall_predictions[0])):\n",
    "                class_confidence_scores = [onevall_predictions[index][sample] for index in range(len(self.classes))]\n",
    "                predictions.append(self.classes[np.argmax(class_confidence_scores)])\n",
    "            return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputes the data by calculatig the mean given a class and replacing all values in that class\n",
    "#that have a feature of \"?\" with the conditional mean. We round the mean to 0 or 1 when the data\n",
    "#is categorical\n",
    "def impute (data, categorical):\n",
    "    class_dfs = []\n",
    "    classes = np.unique(data['target'])\n",
    "    #goes through each label to calculate the conditional mean\n",
    "    for label in classes:\n",
    "        #change the \"?\" to nan so we can compute the mean skipping the nans\n",
    "        data_with_label = data[data['target']==label].replace('?', np.nan).astype('float')\n",
    "        means = data_with_label.mean(skipna=True)\n",
    "        #goes through each feature to replace empty vals with conditional means\n",
    "        #rounds the mean if the data is categorical\n",
    "        for feature in data.columns:\n",
    "            if categorical:\n",
    "                replacement_val = round(means[feature])\n",
    "            else:\n",
    "                replacement_val = means[feature]\n",
    "            data_with_label[feature] = data_with_label[feature].apply(lambda x: replacement_val if math.isnan(x) else x)\n",
    "        class_dfs.append(data_with_label)\n",
    "    return pd.concat(class_dfs).reset_index().drop(columns='index')\n",
    "#split the data into 5 different sets and return each set\n",
    "def validation_sets(data):\n",
    "    sets = []\n",
    "    sample_size = len(data)\n",
    "    class_splits = []\n",
    "    for class_val in np.unique(data['target']):\n",
    "        df_class = data[data['target'] == class_val]\n",
    "        dfs_class = np.array_split(df_class, 5)\n",
    "        class_splits.append(dfs_class)\n",
    "    for index in range(0, 5):\n",
    "        sample_set_list = []\n",
    "        for class_index in range(len(class_splits)):\n",
    "            sample_set_list.append(class_splits[class_index][index])\n",
    "        sample_set = pd.concat(sample_set_list)\n",
    "        sets.append(sample_set.reset_index().drop(columns= 'index'))\n",
    "    return sets\n",
    "\n",
    "#this is the shell for all our models. it takes each set and tests it against the rest of the sets and returns the accuracy\n",
    "#to evaluate our model. it also allows for  different preprocessing, learning rates and number of iterations\n",
    "def k_fold_cross_validation(sets, binary, lr_preprocess, adaline_preprocess, lr_learning_rate, lr_iterations, adaline_learning_rate, adaline_iterations):\n",
    "    lr_hyperparamaters = list(itertools.product(*[lr_learning_rate, lr_iterations]))\n",
    "    lr_scores = [ [] for i in range(len(lr_hyperparamaters)) ]\n",
    "    adaline_hyperparamaters = list(itertools.product(*[adaline_learning_rate, adaline_iterations]))\n",
    "    adaline_scores = [ [] for i in range(len(adaline_hyperparamaters)) ]\n",
    "    for index in range(0, len(sets)):\n",
    "        test_set = sets[index]\n",
    "        #concats the rest of the sets for training\n",
    "        training_set = pd.concat([t_set for (set_index, t_set) in enumerate(sets) if set_index!=index])\n",
    "        y_train = np.array(training_set['target'])\n",
    "        #drop target so its not considered a feature\n",
    "        x_train = training_set.reset_index().drop(columns= ['index','target']).values\n",
    "\n",
    "        y_test = np.array(test_set['target'])\n",
    "        x_test = test_set.reset_index().drop(columns= ['index','target']).values\n",
    "        for index, hyperparamaters in enumerate(lr_hyperparamaters):\n",
    "            lr = LogisticRegression(binary, lr_preprocess=lr_preprocess, learning_rate = hyperparamaters[0], iterations = hyperparamaters[1])\n",
    "            lr.fit(x_train, y_train)\n",
    "            y_predict = lr.predict(x_test)\n",
    "            results = [y_predict[result_index]==actual for result_index, actual in enumerate(y_test)]\n",
    "            lr_scores[index].append(results.count(True)/len(results))\n",
    "        for index, hyperparamaters in enumerate(adaline_hyperparamaters):\n",
    "            adaline = Adaline(binary, adaline_preprocess=adaline_preprocess, learning_rate = hyperparamaters[0], iterations = hyperparamaters[1])\n",
    "            adaline.fit(x_train, y_train)\n",
    "            y_predict = adaline.predict(x_test)\n",
    "            results = [y_predict[result_index]==actual for result_index, actual in enumerate(y_test)]\n",
    "            adaline_scores[index].append(results.count(True)/len(results))\n",
    "    print('logistic regression accuracy:')\n",
    "    for index, hyperparamaters in enumerate(lr_hyperparamaters):\n",
    "        print(f'learning_rate = {hyperparamaters[0]} iterations = {hyperparamaters[1]}: {np.mean(lr_scores[index])}')\n",
    "    print('adaline accuracy:')\n",
    "    for index, hyperparamaters in enumerate(adaline_hyperparamaters):\n",
    "        print(f'learning_rate = {hyperparamaters[0]} iterations = {hyperparamaters[1]}: {np.mean(adaline_scores[index])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing breast cancer data and removing the id column\n",
    "#\"Missing Attribute Values: Denoted by \"?\"\" so we need to impute those\n",
    "breast_cancer = pd.read_csv('breast-cancer-wisconsin.data', sep=\",\", header=None)\n",
    "breast_cancer.columns = [\"id\", \"Clump Thickness\", \"Uniformity of Cell Size\", \"Uniformity of Cell Shape\", \"Marginal Adhesion\",\n",
    "               \"Single Epithelial Cell Size\", \"Bare Nuclei\", \"Bland Chromatin\", \"Normal Nucleoli\", \"Mitoses\", \"target\"]\n",
    "breast_cancer = breast_cancer.drop(columns='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our data labels 2 for benign and 4 for malignant, so let's relabel this to 0 and 1 respectively\n",
    "breast_cancer['target'] = breast_cancer['target'].replace(2, 0).replace(4, 1)\n",
    "\n",
    "#the missing attributes are numerical, so we impute the data by \n",
    "#finding the mean for that meature given the class\n",
    "breast_cancer = impute(breast_cancer, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression accuracy:\n",
      "learning_rate = 0.001 iterations = 50: 0.9685808313835255\n",
      "adaline accuracy:\n",
      "learning_rate = 0.1 iterations = 25: 0.9685502904667148\n"
     ]
    }
   ],
   "source": [
    "#'normalize', 'standardize', None\n",
    "sets = validation_sets(breast_cancer)\n",
    "k_fold_cross_validation(sets, binary = True, lr_preprocess = 'standardize', adaline_preprocess= 'standardize', lr_learning_rate = [0.001], lr_iterations = [50], adaline_learning_rate = [0.1], adaline_iterations = [25])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## House Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing house vote data\n",
    "#Missing Attribute Values: Denoted by \"?\"\n",
    "house_vote = pd.read_csv('house-votes-84.data', sep=\",\", header=None)\n",
    "house_vote.columns = [\"target\", \"handicapped-infants\", \"water-project-cost-sharing\", \"adoption-of-the-budget-resolution\", \"physician-fee-freeze\",\n",
    "               \"el-salvador-aid\", \"religious-groups-in-schools\", \"anti-satellite-test-ban\", \"aid-to-nicaraguan-contras\", \"mx-missile\", \"immigration\",\n",
    "               \"synfuels-corporation-cutback\", \"education-spending\", \"superfund-right-to-sue\", \"crime\", \"duty-free-exports\", \"export-administration-act-south-africa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our labels are democrat and republican, so let's relabel them to 0 and 1 respectively\n",
    "house_vote['target'] = house_vote['target'].replace('democrat', 0).replace('republican', 1)\n",
    "\n",
    "#labels are represented as \"y\" or \"n\", so we change this to \"1\" and \"0\" respectively\n",
    "house_vote = house_vote.replace(\"y\", 1).replace(\"n\", 0)\n",
    "#we also have some \"?\" for missing data. Given that this data is categorical, let's take the mean given the class and round it to 0 or 1 for imputation\n",
    "house_vote = impute(house_vote, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression accuracy:\n",
      "learning_rate = 0.01 iterations = 250: 0.9700323199922238\n",
      "adaline accuracy:\n",
      "learning_rate = 0.001 iterations = 10: 0.9265078612913417\n"
     ]
    }
   ],
   "source": [
    "#'normalize', 'standardize', None\n",
    "sets = validation_sets(house_vote)\n",
    "k_fold_cross_validation(sets, binary = True, lr_preprocess = None, adaline_preprocess= None, lr_learning_rate = [0.01], lr_iterations = [250], adaline_learning_rate = [0.001], adaline_iterations = [10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing iris data\n",
    "iris = pd.read_csv('iris.data', sep=\",\", header=None)\n",
    "iris.columns = [\"sepal length in cm\", \"sepal width in cm\", \"petal length in cm\", \"petal width in cm\", \"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['target'] = iris['target'].replace('Iris-setosa', 0).replace('Iris-versicolor', 1).replace('Iris-virginica', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression accuracy:\n",
      "learning_rate = 0.001 iterations = 250: 0.9733333333333334\n",
      "adaline accuracy:\n",
      "learning_rate = 0.001 iterations = 20: 0.8800000000000001\n"
     ]
    }
   ],
   "source": [
    "#'normalize', 'standardize', None\n",
    "sets = validation_sets(iris)\n",
    "k_fold_cross_validation(sets, binary = False, lr_preprocess = None, adaline_preprocess= 'normalize', lr_learning_rate = [0.001], lr_iterations = [250], adaline_learning_rate = [0.001], adaline_iterations = [20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soybean (small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "soybean = pd.read_csv('soybean-small.data', sep=\",\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "soybean.columns = ['date','plant-stand','precip','temp','hail','crop-hist','area-damaged','severity','seed-tmt','germination','plant-growth','leaves','leafspots-halo','leafspots-marg','leafspot-size','leaf-shread','leaf-malf','leaf-mild','stem','lodging','stem-cankers','canker-lesion','fruiting-bodies','external decay',\n",
    "'mycelium','int-discolor','sclerotia','fruit-pods','fruit spots','seed','mold-growth','seed-discolor','seed-size','shriveling','roots', 'target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "soybean['target'] = soybean['target'].replace('D1', 0).replace('D2', 1).replace('D3', 2).replace('D4', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "soybean = impute(soybean, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "soybean_new = pd.get_dummies(soybean.drop(columns = 'target'), columns =  ['date','plant-stand','precip','temp','hail','crop-hist','area-damaged','severity','seed-tmt','germination','plant-growth','leaves','leafspots-halo','leafspots-marg','leafspot-size','leaf-shread','leaf-malf','leaf-mild','stem','lodging','stem-cankers','canker-lesion','fruiting-bodies','external decay',\n",
    "'mycelium','int-discolor','sclerotia','fruit-pods','fruit spots','seed','mold-growth','seed-discolor','seed-size','shriveling','roots'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "soybean_new['target'] = soybean['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression accuracy:\n",
      "learning_rate = 0.1 iterations = 50: 1.0\n",
      "adaline accuracy:\n",
      "learning_rate = 0.001 iterations = 250: 1.0\n"
     ]
    }
   ],
   "source": [
    "#'normalize', 'standardize', None\n",
    "sets = validation_sets(soybean_new)\n",
    "k_fold_cross_validation(sets, binary = False, lr_preprocess = None, adaline_preprocess= None, lr_learning_rate = [0.1], lr_iterations = [50], adaline_learning_rate = [ 0.001], adaline_iterations = [250])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass = pd.read_csv('glass.data', sep=\",\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass.columns = ['id', 'ri', 'na', 'mg', 'al', 'si', 'k', 'ca', 'ba', 'fe', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass = glass.drop(columns = 'id')\n",
    "glass['target'] = glass['target'].replace(7, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression accuracy:\n",
      "learning_rate = 0.1 iterations = 20: 0.5534006174703849\n",
      "adaline accuracy:\n",
      "learning_rate = 0.001 iterations = 500: 0.49866069331185614\n"
     ]
    }
   ],
   "source": [
    "#'normalize', 'standardize', None\n",
    "sets = validation_sets(glass)\n",
    "k_fold_cross_validation(sets, binary = False, lr_preprocess = 'standardize', adaline_preprocess= 'standardize', lr_learning_rate = [0.1], lr_iterations = [20], adaline_learning_rate = [0.001], adaline_iterations = [500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
