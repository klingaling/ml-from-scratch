{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute (data, categorical):\n",
    "    class_dfs = []\n",
    "    classes = np.unique(data['target'])\n",
    "    #goes through each label to calculate the conditional mean\n",
    "    for label in classes:\n",
    "        #change the \"?\" to nan so we can compute the mean skipping the nans\n",
    "        data_with_label = data[data['target']==label].replace('?', np.nan).astype('float')\n",
    "        means = data_with_label.mean(skipna=True)\n",
    "        #goes through each feature to replace empty vals with conditional means\n",
    "        #rounds the mean if the data is categorical\n",
    "        for feature in data.columns:\n",
    "            if categorical:\n",
    "                replacement_val = round(means[feature])\n",
    "            else:\n",
    "                replacement_val = means[feature]\n",
    "            data_with_label[feature] = data_with_label[feature].apply(lambda x: replacement_val if math.isnan(x) else x)\n",
    "        class_dfs.append(data_with_label)\n",
    "    return pd.concat(class_dfs).reset_index().drop(columns='index')\n",
    "#split the data into 5 different sets and return each set\n",
    "def validation_sets(data):\n",
    "    sets = []\n",
    "    sample_size = len(data)\n",
    "    class_splits = []\n",
    "    for class_val in np.unique(data['target']):\n",
    "        df_class = data[data['target'] == class_val]\n",
    "        dfs_class = np.array_split(df_class, 5)\n",
    "        class_splits.append(dfs_class)\n",
    "    for index in range(0, 5):\n",
    "        sample_set_list = []\n",
    "        for class_index in range(len(class_splits)):\n",
    "            sample_set_list.append(class_splits[class_index][index])\n",
    "        sample_set = pd.concat(sample_set_list)\n",
    "        sets.append(sample_set.reset_index().drop(columns= 'index'))\n",
    "    return sets\n",
    "\n",
    "#this is the shell for all our models. it takes each set and tests it against the rest of the sets and returns the accuracy\n",
    "#to evaluate our model. it also allows for  different preprocessing, layer values/sizes, learning rates, epochs and momentum\n",
    "def k_fold_cross_validation(sets, preprocess, layers, max_hidden_layers, outputs, learning_rate, epochs, momentum):\n",
    "    #the below creates a list of all the possible permutations of the hidden units for each layer for each number of layers\n",
    "    layers_list = []\n",
    "    for layer_number in range(max_hidden_layers):\n",
    "        layer_list = list(itertools.combinations(layers, layer_number+1))\n",
    "        layer_list = [list(vals)+[outputs] for vals in layer_list]\n",
    "        layers_list+=layer_list\n",
    "    hyperparamaters = list(itertools.product(*[layers_list, learning_rate, epochs, momentum]))\n",
    "    hyperparamaters = [list(vals) for vals in hyperparamaters]\n",
    "    scores = [[] for i in range(len(hyperparamaters))]\n",
    "    for index in range(0, len(sets)):\n",
    "        test_set = sets[index]\n",
    "        #concats the rest of the sets for training\n",
    "        training_set = pd.concat([t_set for (set_index, t_set) in enumerate(sets) if set_index!=index])\n",
    "        labels = len(np.unique(training_set['target']))\n",
    "        y= test_set['target']\n",
    "        x_train = training_set.reset_index().drop(columns= ['index','target']).values\n",
    "        x_test = test_set.reset_index().drop(columns= ['index','target']).values\n",
    "        #one hot encode the labels if this is a multiclass problem\n",
    "        if labels>2:\n",
    "            y_train= (np.eye(labels)[training_set['target']])\n",
    "            y_test= (np.eye(labels)[test_set['target']])\n",
    "        else:\n",
    "            y_train = np.array(training_set['target']).reshape(-1,1)\n",
    "            y_test = np.array(test_set['target']).reshape(-1,1)\n",
    "        for index, hyperparamater in enumerate(hyperparamaters):\n",
    "            hyperparamater = list(hyperparamater)\n",
    "#this was my original approach to finding the optimal number of hidden layers and units. I give detail on this in my paper\n",
    "#thought it might be interesting to leave this here commented out\n",
    "#             if dynamic_nodes:\n",
    "#                 hidden_layers = len(hyperparamater[0])-1\n",
    "#                 current_layers = [1]*hidden_layers + [hyperparamater[0][-1]]\n",
    "#                 best_layers = current_layers\n",
    "#                 mlp = MLP(preprocess=preprocess, layers = current_layers, learning_rate = hyperparamater[1], epochs = hyperparamater[2], momentum = hyperparamater[3])\n",
    "#                 mlp.fit(x_train, y_train)\n",
    "#                 y_predict = mlp.predict(x_test)\n",
    "#                 results = [y_predict[result_index]==actual for result_index, actual in enumerate(y)]\n",
    "#                 best_score = results.count(True)/len(results)\n",
    "                \n",
    "#                 for layer_index in range(hidden_layers):\n",
    "#                     decrease = 0\n",
    "#                     current_layers = best_layers\n",
    "#                     while decrease <=5:\n",
    "#                         current_layers[layer_index]+=1\n",
    "#                         print(current_layers)\n",
    "#                         mlp = MLP(preprocess=preprocess, layers = current_layers, learning_rate = hyperparamater[1], epochs = hyperparamater[2], momentum = hyperparamater[3])\n",
    "#                         mlp.fit(x_train, y_train)\n",
    "#                         y_predict = mlp.predict(x_test)\n",
    "#                         results = [y_predict[result_index]==actual for result_index, actual in enumerate(y)]\n",
    "#                         current_score = results.count(True)/len(results)\n",
    "#                         print(current_score)\n",
    "#                         if current_score>best_score:\n",
    "#                             best_layers = current_layers\n",
    "#                             best_score = current_score\n",
    "#                             decrease = 0\n",
    "#                         else:\n",
    "#                             decrease+=1\n",
    "#                 print(best_layers)\n",
    "#                 hyperparamater[0] = best_layers\n",
    "#                 hyperparamaters[index][0] = best_layers\n",
    "            mlp = MLP(preprocess=preprocess, layers = hyperparamater[0], learning_rate = hyperparamater[1], epochs = hyperparamater[2], momentum = hyperparamater[3])\n",
    "            mlp.fit(x_train, y_train)\n",
    "            y_predict = mlp.predict(x_test)\n",
    "            results = [y_predict[result_index]==actual for result_index, actual in enumerate(y)]\n",
    "            scores[index].append(results.count(True)/len(results))\n",
    "    print('MLP accuracy:')\n",
    "    for index, hyperparamaters in enumerate(hyperparamaters):\n",
    "        print(f'layers = {hyperparamaters[0]} learning_rate = {hyperparamaters[1]} epochs = {hyperparamaters[2]} momentum = {hyperparamaters[3]}: {np.mean(scores[index])}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is our class for a multilayer perceptron\n",
    "class MLP():\n",
    "    #init function for initializing the variables we will need across epochs for training and prediction \n",
    "    def __init__(self, preprocess, layers, learning_rate, epochs, momentum=0):\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.momentum = momentum\n",
    "        self.weights = []\n",
    "        self.weight_updates = [[] for _ in range(len(layers))]\n",
    "        self.prior_weight_updates = [[] for _ in range(len(layers))]\n",
    "        self.bias_updates = [[] for _ in range(len(layers))]\n",
    "        self.prior_bias_updates = [[] for _ in range(len(layers))]\n",
    "        self.biases = []\n",
    "        self.outputs = [[] for _ in range(len(layers))]\n",
    "        self.inputs = [[] for _ in range(len(layers))]\n",
    "        self.units = [[] for _ in range(len(layers))]\n",
    "        self.lr_preprocess = preprocess\n",
    "        \n",
    "    #here the weights are initialized to a random number between [-0.01,0.01] the size of the weight matrices correspond\n",
    "    #to the size of the previous layer and the size of the next layer: shape = (previous, next)\n",
    "    def initialize_weights (self, feature_count):\n",
    "        previous = None\n",
    "        for index, layer in enumerate(self.layers):\n",
    "            if previous:\n",
    "                initialized_weight = np.random.uniform(low=-0.01, high=0.01, size=(previous, layer))\n",
    "                initialized_bias = np.random.uniform(low=-0.01, high=0.01, size=layer)\n",
    "            else:\n",
    "                initialized_weight = np.random.uniform(low=-0.01, high=0.01, size=(feature_count, layer))\n",
    "                initialized_bias = np.random.uniform(low=-0.01, high=0.01, size=layer)\n",
    "            previous = layer\n",
    "            self.weights.append(initialized_weight)\n",
    "            self.biases.append(initialized_bias)\n",
    "    #the sigmoid function is used to compute the hidden units and the output values for binary classification\n",
    "    def sigmoid(self, linear_function):\n",
    "        return 1/(1+ np.exp(-linear_function))\n",
    "    #the softmax function is used to compute the output values for multiclass problems\n",
    "    def softmax(self, linear_function):\n",
    "        return np.exp(linear_function) / np.sum(np.exp(linear_function),axis=1,keepdims=True)\n",
    "    #the derivative of the sigmoid function is a required variable for the weight updates of the hidden layers\n",
    "    def sigmoid_deriv(self, linear_function):\n",
    "        sigmoid_val = self.sigmoid(linear_function)\n",
    "        return sigmoid_val*(1-sigmoid_val)\n",
    "    #this is not used but it is the derivative of the softmax function\n",
    "    def softmax_deriv(self, x):\n",
    "        return x*(1-x)\n",
    "    #this function forward propogates the samples through the network given the current weights and computes the output\n",
    "    #the output is computed using sigmoid for binary and softmax for multiclass\n",
    "    def forward_prop(self, x):\n",
    "        for index in range(len(self.layers)):\n",
    "            #first hidden layer\n",
    "            if index ==0:\n",
    "                self.units[index] = np.array(x)\n",
    "                self.inputs[index] = np.dot(x, self.weights[index]) + self.biases[index]\n",
    "                self.outputs[index] = self.sigmoid(self.inputs[index])\n",
    "            #output layer\n",
    "            elif index == len(self.layers)-1:\n",
    "                self.units[index] = np.array(self.outputs[index-1])\n",
    "                self.inputs[index] = np.dot(self.outputs[index-1], self.weights[index]) + self.biases[index]\n",
    "                if self.layers[index]>1:\n",
    "                    self.outputs[index] = self.softmax(self.inputs[index])\n",
    "                else:\n",
    "                    self.outputs[index] = self.sigmoid(self.inputs[index])\n",
    "            #additional hidden layers\n",
    "            else:\n",
    "                self.units[index] = self.outputs[index-1]\n",
    "                self.inputs[index] = np.dot(self.outputs[index-1], self.weights[index]) + self.biases[index]\n",
    "                self.outputs[index] = self.sigmoid(self.inputs[index])\n",
    "    #this function backpropogates \n",
    "    def back_prop(self, x, y):\n",
    "        y_pred = np.array(self.outputs[-1])\n",
    "        diff = (y_pred - y)\n",
    "        for index in range(len(self.layers)-1, -1, -1):\n",
    "            #output layer\n",
    "            if index == len(self.layers)-1:\n",
    "                error = diff\n",
    "                self.prior_weight_updates[index] = self.weight_updates[index]\n",
    "                self.prior_bias_updates[index] = self.bias_updates[index]\n",
    "                self.weight_updates[index] = self.outputs[index-1].T.dot((error))\n",
    "                self.bias_updates[index] = np.sum(error, axis=0)\n",
    "            #additional hidden layers\n",
    "            elif(index!=0):\n",
    "                error = np.multiply((error).dot(self.weights[index+1].T),self.sigmoid_deriv(self.inputs[index]))\n",
    "                self.prior_weight_updates[index] = self.weight_updates[index]\n",
    "                self.prior_bias_updates[index] = self.bias_updates[index]\n",
    "                self.weight_updates[index] = np.dot(self.units[index].T, error)\n",
    "                self.bias_updates[index] = np.sum(error, axis=0)\n",
    "            #first hidden layer\n",
    "            else:\n",
    "                self.prior_weight_updates[index] = self.weight_updates[index]\n",
    "                self.prior_bias_updates[index] = self.bias_updates[index]\n",
    "                self.weight_updates[index] = np.dot(self.units[index].T, np.multiply((error).dot(self.weights[index+1].T),self.sigmoid_deriv(self.inputs[index])))\n",
    "                self.bias_updates[index] = np.sum((error).dot(self.weights[index+1].T)*self.sigmoid_deriv(self.inputs[index]), axis=0)\n",
    "        #updating weights allowing for momentum if specified\n",
    "        for index in range(len(self.weights)):\n",
    "            weight_update = self.weight_updates[index]*self.learning_rate\n",
    "            bias_update = self.bias_updates[index]*self.learning_rate\n",
    "            if self.momentum > 0 and len(self.prior_weight_updates[index])!=0:\n",
    "                weight_update+=self.prior_weight_updates[index]*self.momentum*self.learning_rate\n",
    "                bias_update+=self.prior_bias_updates[index]*self.momentum*self.learning_rate\n",
    "            self.weights[index] -= weight_update\n",
    "            self.biases[index] -= bias_update\n",
    "    #training function utilizing a batch learning approach, also allows for preprocessing of features\n",
    "    #training involved forward prop and back prop for the specified number of epochs\n",
    "    def fit(self, x, y):\n",
    "        x= self.preprocess_features(x)\n",
    "        self.initialize_weights(x.shape[1])\n",
    "        for epoch in range(self.epochs):\n",
    "            n_samples = x.shape[0]\n",
    "            try:\n",
    "                batch_args = np.random.choice(n_samples, 50, replace = False)\n",
    "            except Exception:\n",
    "                batch_args = np.random.choice(n_samples, 32, replace = False)\n",
    "            x_batch = x[batch_args]\n",
    "            y_batch = y[batch_args]\n",
    "            self.forward_prop(x_batch)\n",
    "            self.back_prop(x_batch,y_batch)\n",
    "    #different types of preprocessing for the features, default = none\n",
    "    def preprocess_features(self, x):\n",
    "        if self.lr_preprocess == 'standardize':\n",
    "            return (x-np.mean(x,axis=0))/np.std(x,axis= 0)\n",
    "        elif self.lr_preprocess == 'normalize':\n",
    "            return (((x-np.min(x, axis=0))/(np.max(x, axis=0)-np.min(x, axis=0)))*-1)+1\n",
    "        else:\n",
    "            return x\n",
    "    #this is the predict method for binary classification, it uses the sigmoid approach\n",
    "    def predict_binary(self, x):\n",
    "        output= []\n",
    "        output.append(x)\n",
    "        for index in range(len(self.layers)):\n",
    "            if index == len(self.layers)-1:\n",
    "                output.append(self.sigmoid(np.dot(output[index], self.weights[index]) + self.biases[index]))\n",
    "            else:\n",
    "                output.append(self.sigmoid(np.dot(output[index], self.weights[index]) + self.biases[index]))\n",
    "        out = [1 if x>=0.5 else 0 for x in output[-1]]\n",
    "        return out\n",
    "    #this is the predict method for multiclass classification, it uses the softmax approach\n",
    "    def predict_multiclass(self, x):\n",
    "        output= []\n",
    "        output.append(x)\n",
    "        for index in range(len(self.layers)):\n",
    "            if index == len(self.layers)-1:\n",
    "                output.append(self.softmax(np.dot(output[index], self.weights[index]) + self.biases[index]))\n",
    "            else:\n",
    "                output.append(self.sigmoid(np.dot(output[index], self.weights[index]) + self.biases[index]))\n",
    "        out = [np.argmax(x) for x in output[-1]]\n",
    "        return out\n",
    "    #this is the wrapper method for predictions, it checks if the problem is binary or multiclass to choose the right prediction approach\n",
    "    def predict(self, x):\n",
    "        x= self.preprocess_features(x)\n",
    "        x=np.array(x)\n",
    "        if (self.layers[-1]>1):\n",
    "            out = self.predict_multiclass(x)\n",
    "        else:\n",
    "            out = self.predict_binary(x)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing iris data\n",
    "iris = pd.read_csv('iris.data', sep=\",\", header=None)\n",
    "iris.columns = [\"sepal length in cm\", \"sepal width in cm\", \"petal length in cm\", \"petal width in cm\", \"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['target'] = iris['target'].replace('Iris-setosa', 0).replace('Iris-versicolor', 1).replace('Iris-virginica', 2)\n",
    "#labels = len(np.unique(iris['target']))\n",
    "#y = (np.eye(labels)[iris['target']])\n",
    "#x = iris.drop(columns = 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP accuracy:\n",
      "layers = [30, 3] learning_rate = 0.01 epochs = 500 momentum = 0.6: 0.9800000000000001\n",
      "layers = [8, 3] learning_rate = 0.01 epochs = 500 momentum = 0.6: 0.9866666666666667\n",
      "layers = [30, 8, 3] learning_rate = 0.01 epochs = 500 momentum = 0.6: 0.8933333333333333\n"
     ]
    }
   ],
   "source": [
    "sets = validation_sets(iris)\n",
    "k_fold_cross_validation(sets, preprocess = None, layers= [30, 8], max_hidden_layers = 2, outputs = 3, learning_rate=[0.01], epochs = [500], momentum = [0.6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing breast cancer data and removing the id column\n",
    "#\"Missing Attribute Values: Denoted by \"?\"\" so we need to impute those\n",
    "breast_cancer = pd.read_csv('breast-cancer-wisconsin.data', sep=\",\", header=None)\n",
    "breast_cancer.columns = [\"id\", \"Clump Thickness\", \"Uniformity of Cell Size\", \"Uniformity of Cell Shape\", \"Marginal Adhesion\",\n",
    "               \"Single Epithelial Cell Size\", \"Bare Nuclei\", \"Bland Chromatin\", \"Normal Nucleoli\", \"Mitoses\", \"target\"]\n",
    "breast_cancer = breast_cancer.drop(columns='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our data labels 2 for benign and 4 for malignant, so let's relabel this to 0 and 1 respectively\n",
    "breast_cancer['target'] = breast_cancer['target'].replace(2, 0).replace(4, 1)\n",
    "\n",
    "#the missing attributes are numerical, so we impute the data by \n",
    "#finding the mean for that meature given the class\n",
    "breast_cancer = impute(breast_cancer, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP accuracy:\n",
      "layers = [9, 1] learning_rate = 0.01 epochs = 250 momentum = 0: 0.9685808313835255\n"
     ]
    }
   ],
   "source": [
    "sets = validation_sets(breast_cancer)\n",
    "k_fold_cross_validation(sets, preprocess = 'standardize', layers= [9], max_hidden_layers = 2, outputs=1, learning_rate=[0.01], epochs = [250], momentum=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing house vote data\n",
    "#Missing Attribute Values: Denoted by \"?\"\n",
    "house_vote = pd.read_csv('house-votes-84.data', sep=\",\", header=None)\n",
    "house_vote.columns = [\"target\", \"handicapped-infants\", \"water-project-cost-sharing\", \"adoption-of-the-budget-resolution\", \"physician-fee-freeze\",\n",
    "               \"el-salvador-aid\", \"religious-groups-in-schools\", \"anti-satellite-test-ban\", \"aid-to-nicaraguan-contras\", \"mx-missile\", \"immigration\",\n",
    "               \"synfuels-corporation-cutback\", \"education-spending\", \"superfund-right-to-sue\", \"crime\", \"duty-free-exports\", \"export-administration-act-south-africa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our labels are democrat and republican, so let's relabel them to 0 and 1 respectively\n",
    "house_vote['target'] = house_vote['target'].replace('democrat', 0).replace('republican', 1)\n",
    "\n",
    "#labels are represented as \"y\" or \"n\", so we change this to \"1\" and \"0\" respectively\n",
    "house_vote = house_vote.replace(\"y\", 1).replace(\"n\", 0)\n",
    "#we also have some \"?\" for missing data. Given that this data is categorical, let's take the mean given the class and round it to 0 or 1 for imputation\n",
    "house_vote = impute(house_vote, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP accuracy:\n",
      "layers = [3, 1] learning_rate = 0.01 epochs = 750 momentum = 0: 0.9700323199922238\n"
     ]
    }
   ],
   "source": [
    "sets = validation_sets(house_vote)\n",
    "\n",
    "k_fold_cross_validation(sets, preprocess = None, layers= [3], max_hidden_layers = 2, outputs=1, learning_rate=[0.01], epochs = [750], momentum=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass = pd.read_csv('glass.data', sep=\",\", header=None)\n",
    "glass.columns = ['id', 'ri', 'na', 'mg', 'al', 'si', 'k', 'ca', 'ba', 'fe', 'target']\n",
    "glass = glass.drop(columns = 'id')\n",
    "glass['target'] = glass['target'].replace(7, 0).replace(6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP accuracy:\n",
      "layers = [30, 6] learning_rate = 0.01 epochs = 500 momentum = 0.75: 0.5830690627202254\n"
     ]
    }
   ],
   "source": [
    "sets = validation_sets(glass)\n",
    "k_fold_cross_validation(sets, preprocess = 'standardize', layers= [30], max_hidden_layers = 3, outputs=6, learning_rate=[0.01], epochs = [500], momentum = [0.75])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "soybean = pd.read_csv('soybean-small.data', sep=\",\", header=None)\n",
    "soybean.columns = ['date','plant-stand','precip','temp','hail','crop-hist','area-damaged','severity','seed-tmt','germination','plant-growth','leaves','leafspots-halo','leafspots-marg','leafspot-size','leaf-shread','leaf-malf','leaf-mild','stem','lodging','stem-cankers','canker-lesion','fruiting-bodies','external decay',\n",
    "'mycelium','int-discolor','sclerotia','fruit-pods','fruit spots','seed','mold-growth','seed-discolor','seed-size','shriveling','roots', 'target']\n",
    "soybean['target'] = soybean['target'].replace('D1', 0).replace('D2', 1).replace('D3', 2).replace('D4', 3)\n",
    "soybean = impute(soybean, True)\n",
    "soybean_new = pd.get_dummies(soybean.drop(columns = 'target'), columns =  ['date','plant-stand','precip','temp','hail','crop-hist','area-damaged','severity','seed-tmt','germination','plant-growth','leaves','leafspots-halo','leafspots-marg','leafspot-size','leaf-shread','leaf-malf','leaf-mild','stem','lodging','stem-cankers','canker-lesion','fruiting-bodies','external decay',\n",
    "'mycelium','int-discolor','sclerotia','fruit-pods','fruit spots','seed','mold-growth','seed-discolor','seed-size','shriveling','roots'])\n",
    "soybean_new['target'] = np.array(soybean['target'], dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP accuracy:\n",
      "layers = [15, 4] learning_rate = 0.01 epochs = 250 momentum = 0: 1.0\n"
     ]
    }
   ],
   "source": [
    "sets = validation_sets(soybean_new)\n",
    "k_fold_cross_validation(sets, preprocess = None, layers= [15],max_hidden_layers=1, outputs = 4, learning_rate=[0.01], epochs = [250], momentum = [0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
